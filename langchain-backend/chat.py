import json
import logging
import re
from typing import List, Optional

import asyncio
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, SystemMessage, AIMessage
from tavily import TavilyClient
from pydantic import BaseModel
from dotenv import load_dotenv
import os
from enum import Enum

from prompts import (
    ROUTER_SYSTEM_PROMPT, 
    SELECT_SYSTEM_PROMPT, SELECT_USER_PROMPT,
    ANSWER_SYSTEM_PROMPT, ANSWER_USER_PROMPT,
    CHIT_CHAT_SYSTEM_PROMPT,
    HYBRID_SYSTEM_PROMPT, HYBRID_USER_PROMPT,
    WEB_SEARCH_SYSTEM_PROMPT, WEB_SEARCH_USER_PROMPT,
    REFLECTION_SYSTEM_PROMPT, REFLECTION_USER_PROMPT,
    ALQAC_ANSWER_SYSTEM_PROMPT
)

load_dotenv()
logging.basicConfig(level=logging.INFO)

# --- Configuration ---
CHAT_MODEL = os.getenv("CHAT_MODEL", "JunHowie/Qwen3-4B-GPTQ-Int4") 
BASE_URL = os.getenv("URL", "http://localhost:8000/v1")
API_KEY = os.getenv("API_KEY", "EMPTY")
RERANK_THRESHOLD = 0.75

# --- Helper Functions ---
def clean_reasoning_output(text: str) -> str:
    if not text: return ""
    # Xóa nội dung trong thẻ <think>
    pattern = r"<think>.*?</think>"
    cleaned_text = re.sub(pattern, "", text, flags=re.DOTALL)
    # Xóa luôn thẻ nếu nó bị sót (ví dụ model chưa đóng thẻ)
    cleaned_text = cleaned_text.replace("<think>", "").replace("</think>", "")
    return cleaned_text.strip()

def format_law_docs_for_prompt(docs):
    blocks = []
    for d in docs:
        id = d.get('id', '')
        source = d.get('source', '')
        title = d.get('title', '')
        content = d.get('content', '')
        hierarchy_path = d.get('hierarchy_path', '')
        
        # Thêm dòng phân cách (---) để tách ID khỏi nội dung ngữ nghĩa
        if source == 'vbqppl':
            doc_id = d.get('doc_id', '')
            blocks.append(f"""
            [INTERNAL_ID: {id}]
            TÊN_VĂN_BẢN: Văn bản {doc_id} {title}
            ĐƯỜNG_DẪN: {hierarchy_path}
            NỘI_DUNG: {content}
            --------------------
            """)
        else:
            blocks.append(f"""
            [INTERNAL_ID: {id}]
            TÊN_VĂN_BẢN: {title}
            NỘI_DUNG: {content}
            --------------------
            """)
            
    return "\n".join(blocks)

def parse_selected_ids(llm_output: str) -> List[str]:
    """Extract JSON list from LLM output even if it contains extra text"""
    try:
        # Tìm pattern mảng JSON: ["..."]
        match = re.search(r'\[.*?\]', llm_output, re.DOTALL)
        if match:
            json_str = match.group(0)
            return json.loads(json_str)
        return []
    except Exception:
        logging.error(f"Failed to parse IDs from: {llm_output}")
        return []

async def reflect_query(llm_fast, message: str, history: List[dict]) -> tuple[List[str], str]:
    """Generates extracted queries and a specific rerank query from message history."""
    # Build context từ history
    chat_history_msgs = []
    # Lấy tối đa 4 tin nhắn gần nhất để mô hình không bị loạn context
    for h in history[-4:]: 
        if h['role'] == 'user':
            chat_history_msgs.append(HumanMessage(content=h['content']))
        else:
            chat_history_msgs.append(AIMessage(content=h['content']))
    
    # Gọi LLM
    queries = [message]
    rerank_query = message
    
    try:
        reflection_res = await llm_fast.ainvoke([
            SystemMessage(content=REFLECTION_SYSTEM_PROMPT), 
            *chat_history_msgs,
            HumanMessage(content=REFLECTION_USER_PROMPT.format(question=message))
        ])
        
        # Clean & Parse JSON
        raw_content = clean_reasoning_output(reflection_res.content)
        match = re.search(r'\[.*?\]', raw_content, re.DOTALL)
        
        if match:
            queries = json.loads(match.group(0))
            # Cập nhật query cho rerank
            if len(queries) > 0:
                rerank_query = queries[0]
                
            logging.info(f"Reflected Contextual Queries: {queries}")
            logging.info(f"Selected Query for Rerank: {rerank_query}")
        
        else:
            queries = [raw_content.strip()]
            rerank_query = queries[0] # Fallback lấy raw nếu ko phải JSON

    except Exception as e:
        logging.error(f"Reflection failed: {e}")
        queries = [message]
        rerank_query = message
        
    return queries, rerank_query

# --- Data Models ---
class ChatMode(str, Enum):
    AUTO = "auto"      
    LAW_DB = "law_db"   
    WEB = "web"         
    HYBRID = "hybrid"   

class ChatRequest(BaseModel):
    message: str
    history: Optional[List[dict]] = []
    mode: ChatMode = ChatMode.AUTO 

class ChatRouter:
    def __init__(self):
        self.llm = ChatOpenAI(
            base_url=BASE_URL,
            api_key=API_KEY,
            model=CHAT_MODEL,
            temperature=0.0, # Giữ nhiệt độ thấp nhất để nhất quán
            max_tokens=1024
        )

    async def route(self, query: str, history: List[dict]) -> str:
        # 1. Format History thành chuỗi text để đưa vào context
        # Lấy 2 lượt hội thoại gần nhất để tiết kiệm token nhưng đủ context
        context_str = ""
        if history:
            recent_history = history[-2:] 
            for msg in recent_history:
                role = "User" if msg['role'] == 'user' else "Bot"
                content = msg['content']
                # Cắt ngắn content nếu quá dài để tránh nhiễu router
                if len(content) > 100: 
                    content = content[:100] + "..."
                context_str += f"{role}: {content}\n"
        
        if not context_str:
            context_str = "Không có lịch sử."

        # 2. Tạo Prompt hoàn chỉnh
        # Lưu ý: Ta đưa Context và Input vào user message
        user_input_content = f"""
            LỊCH SỬ CHAT:
            {context_str}

            INPUT HIỆN TẠI:
            "{query}"

            Hãy phân loại INPUT HIỆN TẠI:
            """

        messages = [
            SystemMessage(content=ROUTER_SYSTEM_PROMPT),
            HumanMessage(content=user_input_content)
        ]
        
        # 3. Gọi LLM
        res = await self.llm.ainvoke(messages)
        
        raw_content = res.content
        intent = clean_reasoning_output(raw_content).strip().upper()
        
        logging.info(f"Router Input: {query} | ContextLen: {len(history)} | Output: {intent}")

        if "NON_LEGAL" in intent or "NON LEGAL" in intent:
            return "NON_LEGAL"
        
        return "LEGAL"

class WebSearchEngine:
    def __init__(self):
        api_key = os.getenv("TAVILY_API_KEY")
        if not api_key:
            logging.warning("TAVILY_API_KEY not found in .env")
        self.client = TavilyClient(api_key=api_key)

    def search(self, query: str, top_k=50) -> List[dict]:
        """
        Thực hiện search qua Tavily API.
        """
        try:
            logging.info(f"Tavily searching for: {query}")
            
            # Thêm ngữ cảnh "pháp luật Việt Nam" để kết quả chính xác hơn
            # nếu query người dùng quá ngắn
            search_query = query
            if "việt nam" not in query.lower() and "luật" not in query.lower():
                search_query = f"{query} pháp luật Việt Nam"

            # Gọi API Tavily
            # search_depth="advanced": Tìm sâu hơn, chất lượng hơn (tốn 2 credit)
            # search_depth="basic": Nhanh, tiết kiệm (1 credit)
            response = self.client.search(
                query=search_query,
                search_depth="advanced", 
                max_results=top_k,
                include_answer=False, # Chúng ta tự để LLM trả lời
                include_domains=["thuvienphapluat.vn", "luatvietnam.vn", "baochinhphu.vn"] # (Tuỳ chọn) Giới hạn nguồn uy tín
            )

            results = []
            for res in response.get('results', []):
                results.append({
                    "id": res['url'],           # Dùng URL làm ID
                    "title": res['title'],
                    "content": res['content'],  # Tavily tự tóm tắt nội dung quan trọng
                    "url": res['url'],
                    "source_type": "WEB",       # Đánh dấu nguồn
                    "score": res['score']       # Độ liên quan do Tavily tính
                })
            
            return results

        except Exception as e:
            logging.error(f"Tavily Search Error: {str(e)}")
            # Fallback an toàn: Trả về list rỗng để chain không bị crash
            return []

# --- Helper Logic for Streaming with Citations ---
async def stream_with_citations(llm, messages, rag_engine=None, collection_names=None, context_docs=None):
    """
    Handles streaming response from LLM, parsing <USED_DOCS> tags 
    to separate content from citations.
    """
    buffer = ""
    inside_tag = False
    
    async for chunk in llm.astream(messages):
        content = chunk.content
        if not content:
            continue
        
        buffer += content
        
        # Check for start of tag
        if "<USED_DOCS>" in buffer:
            main_text, remaining = buffer.split("<USED_DOCS>", 1)
            
            if main_text:
                yield json.dumps({"type": "content", "delta": main_text}, ensure_ascii=False) + "\n"
            
            buffer = remaining
            inside_tag = True
            
        elif inside_tag:
            # Inside tag, just accumulate buffer
            pass
            
        else:
            # Normal text handling with safety check for tag start
            if "<" in buffer:
                last_open = buffer.rfind("<")
                potential_tag = buffer[last_open:]
                target = "<USED_DOCS>"
                
                if target.startswith(potential_tag):
                    # Potential partial tag, yield everything before it
                    to_yield = buffer[:last_open]
                    if to_yield:
                        yield json.dumps({"type": "content", "delta": to_yield}, ensure_ascii=False) + "\n"
                    buffer = potential_tag # Keep partial tag in buffer
                else:
                    # Not our tag, yield all
                    yield json.dumps({"type": "content", "delta": buffer}, ensure_ascii=False) + "\n"
                    buffer = ""
            else:
                yield json.dumps({"type": "content", "delta": buffer}, ensure_ascii=False) + "\n"
                buffer = ""

    # End of stream processing
    used_ids = []
    if inside_tag or "<USED_DOCS>" in buffer:
        current_buffer = buffer
        # If we caught the tag start but loop finished
        if "<USED_DOCS>" in current_buffer and not inside_tag:
             _, current_buffer = current_buffer.split("<USED_DOCS>", 1)
        
        # Handle tag closing
        if "</USED_DOCS>" in current_buffer:
            ids_str = current_buffer.split("</USED_DOCS>")[0]
        else:
            ids_str = current_buffer 
            
        clean_ids_str = ids_str.replace(">", "")
        
        # Robust cleaning of IDs
        used_ids = []
        for raw_id in clean_ids_str.split(","):
             clean_id = raw_id.strip()
             # Remove common prefixes model might output
             clean_id = re.sub(r'^\[?INTERNAL_ID:\s*', '', clean_id, flags=re.IGNORECASE)
             clean_id = clean_id.replace(']', '')
             if clean_id:
                 used_ids.append(clean_id)
    else:
        # Flush remaining buffer as text if no tag found
        if buffer:
             yield json.dumps({"type": "content", "delta": buffer}, ensure_ascii=False) + "\n"

    # Send used_docs event
    if used_ids:
        final_used_docs = []
        ids_to_fetch = []
        
        # Helper to find doc in context_docs by ID or URL
        def find_in_context(uid):
            if not context_docs:
                return None
            for d in context_docs:
                # Check various common ID fields
                c_id = str(d.get('id', ''))
                c_url = str(d.get('url', ''))
                c_doc_id = str(d.get('doc_id', ''))
                if uid == c_id or uid == c_url or uid == c_doc_id:
                    return d
            return None

        for uid in used_ids:
            found_doc = find_in_context(uid)
            if found_doc:
                final_used_docs.append(found_doc)
            elif rag_engine:
                 ids_to_fetch.append(uid)
            # If no rag_engine and not in context, we can't do much (or create specific placeholder)
        
        if ids_to_fetch and rag_engine:
             # Load full details from Qdrant for missing IDs
             fetched_docs = await asyncio.to_thread(rag_engine.get_documents_by_ids, ids_to_fetch, collection_names=collection_names)
             final_used_docs.extend(fetched_docs)
        
        # Fallback: If after all extraction we still have nothing (e.g. valid IDs but fetch failed)
        # Note: We might have partial results.
        
        yield json.dumps({"type": "used_docs", "data": final_used_docs}, ensure_ascii=False) + "\n"

    elif context_docs:
        # Fallback to context_docs if no specific citations found
        yield json.dumps({"type": "used_docs", "data": context_docs}, ensure_ascii=False) + "\n"

    logging.info(f"Final Used references: {used_ids}")

class LegalRAGChain:
    def __init__(self):
        # Model cho Selection (Low Temp)
        self.select_llm = ChatOpenAI(
            base_url=BASE_URL,
            api_key=API_KEY,
            model=CHAT_MODEL,
            temperature=0.0,
            max_tokens=1024
        )
        # Model cho Answering (Slightly Higher Temp)
        self.answer_llm = ChatOpenAI(
            base_url=BASE_URL,
            api_key=API_KEY,
            model=CHAT_MODEL,
            temperature=0.3,
            max_tokens=8192,
            streaming=True
        )
        # Model nhanh cho Reflection
        self.llm_fast = ChatOpenAI(
            base_url=BASE_URL, api_key=API_KEY, model=CHAT_MODEL, 
            temperature=0.0, max_tokens=1024 # Tăng token một chút cho suy luận
        )

    async def chat(self, message, history, rag_engine, collection_names: List[str] = None, system_prompt: str = None):
        # --- BƯỚC PRE-PROCESSING: LÀM SẠCH HISTORY ---
        clean_history = []
        for h in history:
            # Copy dict để không ảnh hưởng biến gốc
            msg = h.copy()
            # Chỉ làm sạch tin nhắn của AI (role assistant)
            if msg['role'] == 'assistant':
                msg['content'] = clean_reasoning_output(msg['content'])
            clean_history.append(msg)
        
        # Cập nhật biến history dùng cho các bước sau
        history = clean_history

        # --- BƯỚC 0: MULTI-QUERY REFLECTION ---
        queries, rerank_query = await reflect_query(self.llm_fast, message, history)

         # --- BƯỚC 1: PARALLEL SEARCH (CHỈ SEARCH THÔ) ---
        logging.info(f"Searching Qdrant for {len(queries)} queries parallelly...")
        
        # Mỗi query lấy top 20 thô (chưa rerank)
        tasks = [
            asyncio.to_thread(rag_engine.retrieve, q, top_k=20, collection_names=collection_names) 
            for q in queries
        ]
        
        # Chạy song song
        raw_results_list = await asyncio.gather(*tasks)
        
        # --- BƯỚC 2: DEDUPLICATION & MERGE ---
        unique_docs_map = {}
        
        for batch in raw_results_list:
            for doc in batch:
                point_id = doc['id']
                # Nếu doc đã tồn tại, ta có thể giữ lại hoặc cập nhật
                # Ở bước search thô, điểm qdrant_score không so sánh ngang hàng giữa các query khác nhau được
                # nên ta cứ lấy lần xuất hiện đầu tiên hoặc ghi đè đều ổn.
                if point_id not in unique_docs_map:
                    unique_docs_map[point_id] = doc
        
        # Danh sách ứng viên duy nhất để chuẩn bị Rerank
        merged_candidates = list(unique_docs_map.values())
        logging.info(f"Total unique candidates after merge: {len(merged_candidates)}")

        if not merged_candidates:
             yield json.dumps({"type": "error", "content": "Không tìm thấy văn bản liên quan."}, ensure_ascii=False) + "\n"
             return

        # --- BƯỚC 3: SINGLE RERANK (Rerank 1 lần duy nhất) ---
        # QUAN TRỌNG: Rerank dựa trên câu hỏi gốc (message)
        
        ranked_docs = await asyncio.to_thread(
            rag_engine.rerank, 
            query=rerank_query,  # <--- Dùng message gốc
            sources=merged_candidates, 
            top_k=20 # Lấy top 20 cuối cùng
        )

        # --- BƯỚC 2: KIỂM TRA ĐỘ TIN CẬY (LOGIC MỚI) ---
        # Kiểm tra điểm của văn bản đầu tiên (văn bản khớp nhất)
        top_score = ranked_docs[0].get("rerank_score", 0)
        filtered_docs = []
        skip_llm_filter = False
        top_k = 10
        if top_score > RERANK_THRESHOLD:
            logging.info(f"High Confidence ({top_score:.4f} > {RERANK_THRESHOLD}). Skipping LLM Selection.")
            
            # Logic: Lấy tất cả các docs có điểm > threshold
            # (Hoặc lấy top_k nếu tất cả đều cao để tránh quá tải context)
            filtered_docs = [d for d in ranked_docs if d.get("rerank_score", 0) > RERANK_THRESHOLD]
            
            # Nếu list quá dài, cắt bớt về top_k để tập trung
            filtered_docs = filtered_docs[:top_k]
            
            skip_llm_filter = True
            
        else:
            logging.info(f"Low Confidence ({top_score:.4f} <= {RERANK_THRESHOLD}). Using LLM Selection.")
            skip_llm_filter = False

        # --- BƯỚC 3: XỬ LÝ LỌC (LLM SELECTION) ---
        if not skip_llm_filter:
            docs_for_selection = ranked_docs[:top_k]
            docs_text_block = format_law_docs_for_prompt(docs_for_selection)
            
            # LƯU Ý: Ở bước lọc này, cho LLM xem rerank_query để nó hiểu ngữ cảnh user muốn gì.
            select_messages = [
                SystemMessage(content=SELECT_SYSTEM_PROMPT.format(docs_text=docs_text_block)),
                HumanMessage(content=SELECT_USER_PROMPT.format(question=rerank_query)) 
            ]
            
            try:
                selection_response = await self.select_llm.ainvoke(select_messages)
                selected_ids = parse_selected_ids(selection_response.content)
                if selected_ids:
                    filtered_docs = [d for d in ranked_docs if d['id'] in selected_ids]
                else:
                    filtered_docs = ranked_docs[:top_k]
            except Exception as e:
                logging.error(f"LLM Filter Error: {e}")
                filtered_docs = ranked_docs[:top_k]

        # Trả về Client danh sách nguồn
        yield json.dumps({"type": "sources", "data": filtered_docs}, ensure_ascii=False) + "\n"

        if not filtered_docs:
            yield json.dumps({"type": "content", "delta": "Không tìm thấy quy định phù hợp."}, ensure_ascii=False) + "\n"
            return

        # --- BƯỚC 4: ANSWERING ---
        final_context = format_law_docs_for_prompt(filtered_docs)
        
        
        # Decide which system prompt to use
        current_system_prompt = system_prompt if system_prompt else ANSWER_SYSTEM_PROMPT
        
        # Build chat history messages
        chat_history_msgs = []
        for h in history[-4:]:
            if h['role'] == 'user':
                chat_history_msgs.append(HumanMessage(content=h['content']))
            else:
                chat_history_msgs.append(AIMessage(content=h['content']))
        
        # Dùng câu hỏi gốc (message) để trả lời cho tự nhiên
        answer_messages = [
            SystemMessage(content=current_system_prompt.format(context=final_context))
        ] + chat_history_msgs + [HumanMessage(content=ANSWER_USER_PROMPT.format(question=message))]

        async for chunk in stream_with_citations(self.answer_llm, answer_messages, rag_engine=rag_engine, collection_names=collection_names, context_docs=filtered_docs):
            yield chunk


class WebLawChain:
    def __init__(self):
        self.web_engine = WebSearchEngine()
        self.llm = ChatOpenAI(
            base_url=BASE_URL, api_key=API_KEY, model=CHAT_MODEL,
            temperature=0.3, streaming=True
        )
        self.llm_fast = ChatOpenAI(
            base_url=BASE_URL, api_key=API_KEY, model=CHAT_MODEL, 
            temperature=0.0, max_tokens=1024 
        )

    async def chat(self, message, history, rag_engine):
        # Clean history
        clean_history = []
        for h in history:
            msg = h.copy()
            if msg['role'] == 'assistant':
                msg['content'] = clean_reasoning_output(msg['content'])
            clean_history.append(msg)
        history = clean_history
        
        yield json.dumps({"type": "status", "message": "Đang tìm kiếm thông tin trên internet..."}, ensure_ascii=False) + "\n"
        
        # 1. Reflection: Generate search queries
        queries, rerank_query = await reflect_query(self.llm_fast, message, history)
        
        # 2. Parallel Search
        tasks = [
            asyncio.to_thread(self.web_engine.search, q, top_k=5) 
            for q in queries[:3] # Limit to 3 queries to save credits/time
        ]
        results_list = await asyncio.gather(*tasks)
        
        # 3. Deduplicate
        unique_results = {}
        for batch in results_list:
            for res in batch:
                if res['url'] not in unique_results:
                    unique_results[res['url']] = res
        
        merged_results = list(unique_results.values())
        logging.info(f'Web Results: {len(merged_results)} items')

        if not merged_results:
            yield json.dumps({"type": "content", "delta": "Không tìm thấy thông tin trên internet."}, ensure_ascii=False) + "\n"
            return
            
        # 4. Rerank
        # Use rag_engine.rerank if available (it handles list of dicts)
        reranked_results = await asyncio.to_thread(
            rag_engine.rerank, 
            query=rerank_query, 
            sources=merged_results, 
            top_k=10
        )
        
        # Return sources (only once)
        yield json.dumps({"type": "sources", "data": reranked_results}, ensure_ascii=False) + "\n"
        
        # Build chat history messages
        chat_history_msgs = []
        for h in history[-4:]:
            if h['role'] == 'user':
                chat_history_msgs.append(HumanMessage(content=h['content']))
            else:
                chat_history_msgs.append(AIMessage(content=h['content']))
        
        # Answer with history context
        messages = [
            SystemMessage(content=WEB_SEARCH_SYSTEM_PROMPT.format(web_results=json.dumps(reranked_results, ensure_ascii=False, indent=2)))
        ] + chat_history_msgs + [
            HumanMessage(content=message)
        ]
        
        async for chunk in stream_with_citations(self.llm, messages, context_docs=reranked_results):
            yield chunk

# 3. HybridChain (MỚI: Xử lý cả 2)
class HybridChain:
    def __init__(self):
        self.web_engine = WebSearchEngine()
        self.llm = ChatOpenAI(
            base_url=BASE_URL, api_key=API_KEY, model=CHAT_MODEL,
            temperature=0.2, streaming=True
        )
        self.llm_fast = ChatOpenAI(
            base_url=BASE_URL, api_key=API_KEY, model=CHAT_MODEL, 
            temperature=0.0, max_tokens=1024 
        )

    async def chat(self, message, history, rag_engine):
        # Clean history
        clean_history = []
        for h in history:
            msg = h.copy()
            if msg['role'] == 'assistant':
                msg['content'] = clean_reasoning_output(msg['content'])
            clean_history.append(msg)
        history = clean_history
        
        yield json.dumps({"type": "status", "message": "Đang đối chiếu dữ liệu hệ thống và internet..."}, ensure_ascii=False) + "\n"
        
        # 1. Reflection
        queries, rerank_query = await reflect_query(self.llm_fast, message, history)

        # 2. Parallel retrieval: RAG + Web
        # RAG Search (Multi-query)
        rag_tasks = [
            asyncio.to_thread(rag_engine.retrieve, q, top_k=10) 
            for q in queries
        ]
        # Web Search (Limit queries)
        web_tasks = [
            asyncio.to_thread(self.web_engine.search, q, top_k=5)
            for q in queries[:2]
        ]
        
        all_results = await asyncio.gather(*(rag_tasks + web_tasks))
        
        rag_results_batches = all_results[:len(rag_tasks)]
        web_results_batches = all_results[len(rag_tasks):]
        
        # Deduplicate RAG
        unique_rag = {}
        for batch in rag_results_batches:
            for doc in batch:
                unique_rag[doc['id']] = doc
        
        # Deduplicate Web
        unique_web = {}
        for batch in web_results_batches:
            for doc in batch:
                unique_web[doc['url']] = doc
                
        rag_docs = list(unique_rag.values())
        web_docs = list(unique_web.values())

        # Label source types
        for d in rag_docs: d["source_type"] = "LAW_DB"
        for d in web_docs: d["source_type"] = "WEB"

        # Prioritize LAW_DB by putting them first, then rerank all
        all_docs = rag_docs + web_docs
        
        if all_docs:
            # Rerank merged results to get most relevant using user's initial query (or rerank_query)
            ranked_docs = await asyncio.to_thread(rag_engine.rerank, rerank_query, all_docs, top_k=15)
            # Sort to prioritize LAW_DB within reranked results
            ranked_docs.sort(key=lambda x: (0 if x.get("source_type") == "LAW_DB" else 1, -x.get("rerank_score", 0)))
        else:
            ranked_docs = []

        # Stream sources
        yield json.dumps({"type": "sources", "data": ranked_docs}, ensure_ascii=False) + "\n"

        if not ranked_docs:
             yield json.dumps({"type": "content", "delta": "Không tìm thấy thông tin ở cả kho luật và internet."}, ensure_ascii=False) + "\n"
             return

        # Format context with IDs for citation
        context_blocks = []
        for d in ranked_docs:
            source_label = "[KHO_LUAT]" if d.get("source_type") == "LAW_DB" else "[INTERNET]"
            doc_id = d.get('id', d.get('url', 'unknown'))
            context_blocks.append(f"""
                {source_label}
                [INTERNAL_ID: {doc_id}]
                Tiêu đề: {d.get('title', '')}
                Nội dung: {d.get('content', '')}
                """)
        full_context = "\n---\n".join(context_blocks)

        # Build chat history messages
        chat_history_msgs = []
        for h in history[-4:]:
            if h['role'] == 'user':
                chat_history_msgs.append(HumanMessage(content=h['content']))
            else:
                chat_history_msgs.append(AIMessage(content=h['content']))

        # Answering with history context
        messages = [
            SystemMessage(content=HYBRID_SYSTEM_PROMPT.format(context=full_context))
        ] + chat_history_msgs + [
            HumanMessage(content=message)
        ]

        async for chunk in stream_with_citations(self.llm, messages, rag_engine=rag_engine, context_docs=ranked_docs):
            yield chunk

class ChitChatChain:
    def __init__(self):
        self.llm = ChatOpenAI(
            base_url=BASE_URL,
            api_key=API_KEY,
            model=CHAT_MODEL,
            temperature=0.6,
            streaming=True
        )

    async def chat(self, message, history, rag_engine):
        # --- BƯỚC PRE-PROCESSING: LÀM SẠCH HISTORY ---
        clean_history = []
        for h in history:
            # Copy dict để không ảnh hưởng biến gốc
            msg = h.copy()
            # Chỉ làm sạch tin nhắn của AI (role assistant)
            if msg['role'] == 'assistant':
                msg['content'] = clean_reasoning_output(msg['content'])
            clean_history.append(msg)
        
        # Cập nhật biến history dùng cho các bước sau
        history = clean_history

        messages = [SystemMessage(content=CHIT_CHAT_SYSTEM_PROMPT)]
        
        for h in history[-4:]:
            if h['role'] == 'user':
                messages.append(HumanMessage(content=h['content']))
            else:
                messages.append(AIMessage(content=h['content'])) 
        
        messages.append(HumanMessage(content=message))

        # ChitChat doesn't return sources
        yield json.dumps({"type": "sources", "data": []}, ensure_ascii=False) + "\n"

        async for chunk in self.llm.astream(messages):
            if chunk.content:
                yield json.dumps({"type": "content", "delta": chunk.content}, ensure_ascii=False) + "\n"