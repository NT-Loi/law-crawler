# Vietnamese Law RAG Chatbot

A Retrieval-Augmented Generation (RAG) chatbot for Vietnamese legal documents, built with LangChain, Qdrant, PostgreSQL, and a modern React frontend.

## ğŸ—ï¸ Architecture

![System Architecture](images/image.png)

The system utilizes **Tavily API** to support Web Search and Hybrid Search modes, allowing the chatbot to access up-to-date information from the internet alongside the local legal database.

## ğŸ§  RAG Pipeline Details

### Document Chunking Strategy

The system uses **hierarchical section-based chunking**. Each "Äiá»u" (Article) is the primary chunk unit with its full hierarchy path preserved:

```
VBQPPL Document
â”œâ”€â”€ Pháº§n (Part)
â”‚   â”œâ”€â”€ ChÆ°Æ¡ng (Chapter)
â”‚   â”‚   â”œâ”€â”€ Má»¥c (Section)
â”‚   â”‚   â”‚   â””â”€â”€ Äiá»u (Article) â† Primary chunk unit
```

**Why Äiá»u-level chunking?** Legal articles are semantic units; splitting mid-article loses meaning. Each chunk is embedded with context: `{title}\n{hierarchy_path}\n{content}`.

<!-- ### Complete RAG Pipeline

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                              User Query + History                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                       â”‚
                                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         1. ChatRouter (LLM)                                 â”‚
â”‚                     Classify: LEGAL / NON_LEGAL                             â”‚
â”‚                     (Uses last 2 messages for context)                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚                    â”‚
                         LEGAL               NON_LEGAL
                            â”‚                    â”‚
                            â–¼                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚        LegalRAGChain               â”‚    â”‚  ChitChatChain  â”‚
â”‚                                    â”‚    â”‚  (Direct LLM)   â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚ â”‚ 2. Multi-Query Reflection      â”‚ â”‚
â”‚ â”‚    LLM generates 3 queries:    â”‚ â”‚
â”‚ â”‚    Q1: Contextualized          â”‚ â”‚
â”‚ â”‚    Q2: Legal terminology       â”‚ â”‚
â”‚ â”‚    Q3: Broader domain terms    â”‚ â”‚
â”‚ â”‚    â†’ Q1 used for reranking     â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                 â”‚                  â”‚
â”‚                 â–¼                  â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚ 3. Parallel Hybrid Search      â”‚ â”‚
â”‚ â”‚    3 queries Ã— (Dense + BM25)  â”‚ â”‚
â”‚ â”‚    Qdrant RRF Fusion           â”‚ â”‚
â”‚ â”‚    â†’ 30-60 candidates          â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                 â”‚                  â”‚
â”‚                 â–¼                  â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚ 4. Voyage AI Rerank            â”‚ â”‚
â”‚ â”‚    Query: Q1 (contextualized)  â”‚ â”‚
â”‚ â”‚    â†’ Top 20 scored             â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                 â”‚                  â”‚
â”‚                 â–¼                  â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚ 5. Confidence-Based Filtering  â”‚ â”‚
â”‚ â”‚    Score > 0.75? Use top docs  â”‚ â”‚
â”‚ â”‚    Score â‰¤ 0.75? LLM selects   â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                 â”‚                  â”‚
â”‚                 â–¼                  â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚ 6. Answer Generation (LLM)     â”‚ â”‚
â”‚ â”‚    Context + Last 4 messages   â”‚ â”‚
â”‚ â”‚    â†’ Streaming response        â”‚ â”‚
â”‚ â”‚    â†’ <USED_DOCS> citations     â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      Streaming Response + Rich Citations                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
``` -->

### Pipeline Stage Details

| Stage | Component | Description |
|-------|-----------|-------------|
| **1. Router** | `ChatRouter` | Classifies query as LEGAL/NON_LEGAL using last 2 history messages |
| **2. Reflection** | `REFLECTION_SYSTEM_PROMPT` | Generates 3 search queries; Q1 resolves pronouns from history |
| **3. Search** | `rag.retrieve()` | Hybrid search (dense + BM25) with RRF fusion, runs 3 queries in parallel |
| **4. Rerank** | Voyage AI `rerank-2.5` | Semantic reranking using contextualized Q1 |
| **5. Filter** | Confidence check | Score > 0.75 skips LLM; otherwise LLM selects relevant doc IDs |
| **6. Answer** | `answer_llm` | Generates response with `<USED_DOCS>` citation tags |

### History Support

| Component | History Usage |
|-----------|---------------|
| Router | Last 2 messages (100 chars each) |
| Reflection | Last 4 messages to resolve pronouns |
| Answer LLM | Last 4 messages as conversation context |
| ChitChat | Last 4 messages for natural flow |

<!-- ### Key Design Decisions

| Decision | Rationale |
|----------|-----------|
| Äiá»u-level chunking | Legal articles are semantic units |
| Hierarchy path in embeddings | Same article number differs across chapters |
| Q1 for reranking | Contextualized query captures intent better |
| 0.75 confidence threshold | High scores = clear match, skip LLM for speed |
| 4-message history limit | Balance context vs. token efficiency | -->

## ğŸ“‹ Prerequisites

- **Python 3.10+**
- **Docker & Docker Compose**
- **Node.js 18+** (for frontend)
- **GPU (Optional)**: CUDA-compatible GPU for faster embedding

## ğŸš€ Quick Start

### 1. Clone and Setup

```bash
git clone https://github.com/NT-Loi/law-chatbot.git
cd law-chatbot

# Create virtual environments
cd law-crawler && python -m venv .venv && source .venv/bin/activate
pip install -r requirements.txt
cd ../langchain-backend && python -m venv .venv && source .venv/bin/activate
pip install -r requirements.txt
```

### 2. Start Infrastructure Services

```bash
docker compose up -d
```

This starts:
- **Qdrant** (Vector DB) - `localhost:6333`
- **PostgreSQL** - `localhost:5432`
- **Prometheus** - `localhost:9090`
- **Grafana** - `localhost:3001`

### 3. Configure Environment

Edit `langchain-backend/.env`:

```env
# LLM Configuration
CHAT_MODEL=JunHowie/Qwen3-4B-GPTQ-Int4
URL=http://127.0.0.1:8000/v1
API_KEY=EMPTY

# External APIs
TAVILY_API_KEY=your_tavily_key
VOYAGE_API_KEY=your_voyage_key

# Database
DATABASE_URL=postgresql://root:123456789@localhost:5432/law_database

# Qdrant
QDRANT_HOST=localhost
QDRANT_PORT=6333

# Embedding Model
EMBEDDING_MODEL=AITeamVN/Vietnamese_Embedding
VECTOR_SIZE=1024
MAX_SEQ_LENGTH=2048

# Data Paths
PHAPDIEN_DIR=/path/to/law-chatbot/data/phap_dien
VBQPPL=/path/to/law-chatbot/data/vbqppl_content.json
QA_VBQPPL=/path/to/law-chatbot/data/qa_vbqppl_content.json
```

## ğŸ“š Data Preparation

### Step 1: Prepare PhÃ¡p Äiá»ƒn Data

1. **Download** `BoPhapDienDienTu.zip` from the official source
2. **Unzip** to `law-crawler/BoPhapDienDienTu/`
3. **Run the crawler**:

```bash
cd law-crawler
python phapdien_crawler.py
```

**Output**: `data/phap_dien/Dieu.json`, `data/phap_dien/Chuong.json`

### Step 2: Crawl VBQPPL Documents

This fetches the legal documents referenced in PhÃ¡p Äiá»ƒn:

```bash
cd law-crawler
python vbqppl_crawler.py --input ../data/phap_dien/Dieu.json --output ../data/vbqppl_content.json
```

Options:
- `--phase 1` - Fetch HTML only
- `--phase 2` - Extract only (from cached HTML)
- `--limit N` - Process only N documents

### Step 3: Crawl QA Dataset References

For the evaluation dataset, crawl additional referenced documents:

```bash
cd law-crawler
python qa_dataset_crawler.py
```

**Output**: `data/qa_vbqppl_content.json`

## ğŸ’¾ Data Ingestion

### Ingest into Qdrant (Vector Database)

```bash
cd langchain-backend
python ingest_qdrant.py
```

Creates collections:
- `phapdien_vietnamese_embedding` - PhÃ¡p Äiá»ƒn articles
- `vbqppl_vietnamese_embedding` - VBQPPL document sections

### Ingest into PostgreSQL (Full Documents)

```bash
cd langchain-backend
python ingest_psql.py

# To reset tables first:
python ingest_psql.py --drop
```

## ğŸ–¥ï¸ Running the Backend

### Start vLLM Server (if using local model)

```bash
vllm serve JunHowie/Qwen3-4B-GPTQ-Int4 --host 0.0.0.0 --port 8000 --max-num-seqs 1 --gpu-memory-utilization 0.90 --enforce-eager --trust-remote-code > vllm.log 2>&1
```

### Start FastAPI Backend

```bash
cd langchain-backend
python app.py
# Or with uvicorn:
uvicorn app:app --host 0.0.0.0 --port 8888 --reload
```

API available at `http://localhost:8888`

### API Endpoints

| Endpoint | Method | Description |
|----------|--------|-------------|
| `/` | GET | Health check |
| `/chat` | POST | Chat with RAG |
| `/documents` | GET | List/Search documents |
| `/document/{id}` | GET | Get document by ID |

## ğŸ“Š Evaluation

### Dataset 1: du_lieu_luat_dataset.json

Generate system outputs:
```bash
cd langchain-backend
python eval.py --input ../data/du_lieu_luat_dataset.json --output ../data/evaluation_results.json
```

Compute metrics (retrieval & citation):
```bash
python compute_metrics.py
```

### Dataset 2: ALQAC-2025

1. **Ingest corpus**:
```bash
python ingest_alqac25.py
```

2. **Run evaluation**:
```bash
python eval_alqac25.py --input ../ALQAC-2025/alqac25_train.json --output ../data/alqac25_eval_results.json
```

3. **Compute metrics**:
```bash
python compute_alqac_metrics.py --results ../data/alqac25_eval_results.json
```

### Evaluation Metrics Explained

The system computes **Retrieval Metrics** and **Citation Metrics**:

#### Retrieval Metrics (RAG Pipeline Quality)

| Metric | Formula | Description |
|--------|---------|-------------|
| **Retrieval Recall** | `matched_gt / total_gt` | Percentage of ground truth citations found in retrieved context documents |
| **Retrieval Precision** | `relevant_docs / total_context_docs` | Percentage of retrieved documents that contain relevant citations |

#### Citation Metrics (LLM Answer Quality)

| Metric | Formula | Description |
|--------|---------|-------------|
| **Citation Recall** | `matched_gt / total_gt` | Percentage of ground truth citations the LLM actually cited in its answer |
| **Citation Precision** | `relevant_used / total_used` | Percentage of LLM's citations that are actually relevant |

<!-- #### Matching Methodology

The evaluation uses **fuzzy string matching** (SequenceMatcher ratio â‰¥ 0.5) because:
- Vietnamese legal citations have variations ("Äiá»u 5" vs "Äiá»u 5.")
- Hierarchy paths require reconstruction from document metadata
- Different formatting styles exist across documents -->

<!-- **Citation extraction** parses legal references like:
- "Äiá»u 5 Luáº­t Thanh niÃªn 2020"
- "Nghá»‹ Ä‘á»‹nh 100/2019/NÄ-CP"
- "Khoáº£n 2 Äiá»u 3 ThÃ´ng tÆ° 01/2021/TT-BTP" -->

#### ALQAC-2025 Specific Metrics

| Metric | Description |
|--------|-------------|
| **Answer Accuracy** | Exact match for ÄÃºng/Sai (True/False) and Multiple Choices (A,B,C,D) |
| **Retrieval F1** | `2 Ã— P Ã— R / (P + R)` for context documents |
| **Citation F1** | `2 Ã— P Ã— R / (P + R)` for used documents |

## ğŸ“ Project Structure

```
law-chatbot/
â”œâ”€â”€ docker-compose.yml      # Infrastructure services
â”œâ”€â”€ prometheus.yml          # Prometheus config
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ phap_dien/         # PhÃ¡p Äiá»ƒn data
â”‚   â”‚   â”œâ”€â”€ Dieu.json      # Articles
â”‚   â”‚   â””â”€â”€ Chuong.json    # Chapters
â”‚   â”œâ”€â”€ vbqppl_content.json
â”‚   â”œâ”€â”€ qa_vbqppl_content.json
â”‚   â””â”€â”€ du_lieu_luat_dataset.json
â”œâ”€â”€ ALQAC-2025/            # Evaluation dataset
â”œâ”€â”€ law-crawler/
â”‚   â”œâ”€â”€ phapdien_crawler.py
â”‚   â”œâ”€â”€ vbqppl_crawler.py
â”‚   â”œâ”€â”€ qa_dataset_crawler.py
â”‚   â””â”€â”€ utils.py
â”œâ”€â”€ langchain-backend/
â”‚   â”œâ”€â”€ .env               # Configuration
â”‚   â”œâ”€â”€ app.py             # FastAPI endpoints
â”‚   â”œâ”€â”€ chat.py            # Chat chains
â”‚   â”œâ”€â”€ rag.py             # RAG retrieval
â”‚   â”œâ”€â”€ models.py          # DB models
â”‚   â”œâ”€â”€ prompts.py         # System prompts
â”‚   â”œâ”€â”€ utils.py           # Utility functions
â”‚   â”œâ”€â”€ ingest_qdrant.py
â”‚   â”œâ”€â”€ ingest_psql.py
â”‚   â”œâ”€â”€ ingest_alqac25.py
â”‚   â”œâ”€â”€ eval.py
â”‚   â”œâ”€â”€ eval_alqac25.py
â”‚   â”œâ”€â”€ compute_metrics.py
â”‚   â””â”€â”€ compute_alqac_metrics.py
â””â”€â”€ frontend/              # React frontend
```

## Monitoring (Prometheus & Grafana)

The project includes observability infrastructure for monitoring API performance.

### Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      scrape /metrics      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  FastAPI Backend â”‚  â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚  Prometheus â”‚
â”‚  (port 8888)     â”‚         every 5s          â”‚  (port 9090)â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                      â”‚
                                                      â”‚ data source
                                                      â–¼
                                               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                               â”‚   Grafana   â”‚
                                               â”‚  (port 3001)â”‚
                                               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Prometheus

**Purpose**: Collects time-series metrics from the FastAPI backend.

**Configuration** (`prometheus.yml`):
```yaml
scrape_configs:
  - job_name: 'law-chatbot-backend'
    scrape_interval: 5s
    static_configs:
      - targets: ['host.docker.internal:8888']
```

**Available Metrics** (exposed at `http://localhost:8888/metrics`):
- `http_requests_total` - Total HTTP requests by method, path, status
- `http_request_duration_seconds` - Request latency histogram
- `http_requests_in_progress` - Current concurrent requests

**Access Prometheus UI**: http://localhost:9090

**Example Queries**:
```promql
# Request rate per second
rate(http_requests_total[5m])

# 95th percentile latency
histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m]))

# Error rate
sum(rate(http_requests_total{status=~"5.."}[5m])) / sum(rate(http_requests_total[5m]))
```

### Grafana

**Purpose**: Visualize metrics with customizable dashboards.

**Access Grafana**: http://localhost:3001
- Default credentials: `admin` / `admin`

**Setup Dashboard**:
1. Go to **Connections** â†’ **Data Sources** â†’ **Add data source**
2. Select **Prometheus**
3. Set URL: `http://prometheus:9090`
4. Click **Save & Test**

**Recommended Dashboard Panels**:
| Panel | Query | Description |
|-------|-------|-------------|
| Request Rate | `rate(http_requests_total[1m])` | Requests per second |
| Latency P95 | `histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m]))` | 95th percentile response time |
| Error Rate | `sum(rate(http_requests_total{status=~"5.."}[5m]))` | 5xx errors per second |
| Active Requests | `http_requests_in_progress` | Concurrent connections |

## ğŸ”§ Troubleshooting

### Qdrant Connection Error
```bash
# Check if Qdrant is running
docker ps | grep qdrant
# Restart if needed
docker compose restart qdrant
```

### PostgreSQL Connection Error
```bash
# Check connection
docker exec -it law_postgres psql -U root -d law_database
```

### CUDA Out of Memory
- Reduce `batch_size` in ingestion scripts
- Use smaller embedding model

### Empty Chat Responses
- Verify vLLM server is running
- Check `URL` in `.env` matches vLLM endpoint

### Prometheus Not Collecting Metrics
```bash
# Check if backend exposes metrics
curl http://localhost:8888/metrics

# Check Prometheus targets
# Go to http://localhost:9090/targets
```
